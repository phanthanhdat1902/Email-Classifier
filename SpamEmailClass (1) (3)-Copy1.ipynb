{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "# Create 3 files : hardHam_clean, easyHam_clean, spam_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Game\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import os\n",
    "from text_preprocessing import preprocess_text\n",
    "from text_preprocessing import (\n",
    "    to_lower,\n",
    "    remove_email, \n",
    "    remove_url, \n",
    "    remove_punctuation, \n",
    "    lemmatize_word, \n",
    "    remove_number, \n",
    "    stem_word, \n",
    "    remove_stopword,\n",
    "    remove_whitespace\n",
    ")\n",
    "# Thu vien email.parser dung cho tep file co cau truc gioi tin email http\n",
    "from email.parser import Parser\n",
    "\n",
    "### Lay duong dan den tat ca cac file trong folder\n",
    "def read_files_name(folderPath):\n",
    "    result = []\n",
    "    for dirPath, dirs, files in os.walk(folderPath):\n",
    "        for fileName in files:\n",
    "            fName = os.path.join(dirPath, fileName)\n",
    "            result.append(fName)\n",
    "    return result\n",
    "\n",
    "### Tra ve 3 mang chua danh sach ten file trong thu muc\n",
    "def read_forders_Path():\n",
    "    easyHam = read_files_name('dataset\\easy_ham')\n",
    "    hardHam = read_files_name('dataset\\hard_ham')\n",
    "    spam = read_files_name('dataset\\spam')\n",
    "    return easyHam, hardHam, spam\n",
    "\n",
    "\n",
    "def strip_url(mess):\n",
    "    return re.sub(r'http[s]?://\\S+|www\\.\\S+', ' ', mess)\n",
    "\n",
    "\n",
    "def strip_html(mess):\n",
    "    # clean html_tag\n",
    "    clean = re.compile('<.*?>')\n",
    "    mess = re.sub(clean, ' ', mess)\n",
    "    # clean html_space_white\n",
    "    return re.sub('&nbsp;', ' ', mess)\n",
    "\n",
    "\n",
    "\n",
    "def strip_email_header(message):\n",
    "    message = Parser().parsestr(message)\n",
    "    subject = message.get('subject', '')\n",
    "    body = get_email_body(message)\n",
    "\n",
    "    return '\\n'.join((subject, body))\n",
    "\n",
    "def get_email_body(message):\n",
    "    payloads = message.get_payload()\n",
    "    if isinstance(payloads, list):\n",
    "        return '\\n'.join([get_email_body(message) for message in payloads])\n",
    "    elif isinstance(payloads, str):\n",
    "        return payloads\n",
    "\n",
    "\n",
    "def preprocessing_data(mess):\n",
    "    # processing_function_list = [\n",
    "    #     str.lower,\n",
    "    #     strip_url,\n",
    "    #     remove_url,\n",
    "    #     strip_html,\n",
    "    #     remove_email,\n",
    "    #     remove_number,\n",
    "    #     remove_stopword,\n",
    "    #     remove_punctuation,\n",
    "    #     remove_whitespace\n",
    "\n",
    "    # ]\n",
    "\n",
    "#     preprocess_functions = [strip_email_header, to_lower, remove_email, remove_url,strip_url,strip_html, \n",
    "#                             remove_number, remove_punctuation, remove_stopword, lemmatize_word]\n",
    "    preprocess_functions = [to_lower, remove_email, remove_url, \n",
    "                            remove_number, remove_punctuation, remove_stopword, lemmatize_word]\n",
    "    aftermess = preprocess_text(mess, preprocess_functions)\n",
    "\n",
    "    # return ' '.join(aftermess)\n",
    "    return aftermess\n",
    "\n",
    "def handler():\n",
    "    easyHam,hardHam,spam = read_forders_Path()\n",
    "    count=0\n",
    "#     with open(a[0], 'r', encoding='latin') as fi:\n",
    "#         print(preprocessing_data(fi.read()))\n",
    "    f=open('dataset\\easyHam_clean.txt','a+',encoding='utf-8')\n",
    "#     print(len(easyHam))\n",
    "    for i in easyHam:\n",
    "        try:\n",
    "            with open(i, 'r', encoding='latin') as fi:\n",
    "                temp=preprocessing_data(fi.read())\n",
    "            f.write(temp)\n",
    "#                print(preprocessing_data(fi.read())) \n",
    "            f.write('\\n')\n",
    "        except Exception as e:\n",
    "            print(i + \"Exception :\" + str(e))\n",
    "            count+=1\n",
    "            continue\n",
    "    f.close()\n",
    "    ### hardHam\n",
    "    f=open('dataset\\hardHam_clean.txt','a+',encoding='utf-8')\n",
    "    for i in hardHam:\n",
    "        try:\n",
    "            with open(i, 'r', encoding='latin') as fi:\n",
    "                temp=preprocessing_data(fi.read())\n",
    "            f.write(temp)\n",
    "#                print(preprocessing_data(fi.read())) \n",
    "            f.write('\\n')\n",
    "        except Exception as e:\n",
    "            print(i + \"Exception :\" + str(e))\n",
    "            count+=1\n",
    "            continue\n",
    "    f.close()\n",
    "    \n",
    "    ### spam \n",
    "    f=open('dataset\\spam_clean.txt','a+',encoding='utf-8')\n",
    "    for i in spam:\n",
    "        try:\n",
    "            with open(i, 'r', encoding='latin') as fi:\n",
    "                temp=preprocessing_data(fi.read())\n",
    "            f.write(temp)\n",
    "#                print(preprocessing_data(fi.read())) \n",
    "            f.write('\\n')\n",
    "        except Exception as e:\n",
    "            print(i + \"Exception :\" + str(e))\n",
    "            count+=1\n",
    "            continue\n",
    "    f.close()\n",
    "    print(\"End Game\\n\")\n",
    "#     for i in hardHam:\n",
    "#         f=open('dataset\\hardHam_clean.txt','w+')\n",
    "#         with open(i, 'r', encoding='latin') as fi:\n",
    "#             f.write(preprocessing_data(fi.read()))\n",
    "#         f.write(' ')\n",
    "#     f.close()\n",
    "#     for i in spam:\n",
    "#         f=open('dataset\\spam_clean.txt','w+')\n",
    "#         with open(i, 'r', encoding='latin') as fi:\n",
    "#             f.write(preprocessing_data(fi.read()))\n",
    "#         f.write(' ')\n",
    "#     f.close()\n",
    "if __name__ == \"__main__\":\n",
    "    handler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thống kê tần suất xuất hiện và tính xác suất trước"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viết 1 hàm: input 1 file, output 1 mảng kiểu map (từ, fre)\n",
    "from collections import Counter, defaultdict\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fileName):\n",
    "    with open('dataset\\\\'+fileName, 'r', encoding='utf-8') as fi:\n",
    "            temp=fi.readlines()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(data_train):\n",
    "    result=Counter()\n",
    "    for email in data_train:\n",
    "        word=email.split(' ')\n",
    "        result.update(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readData():\n",
    "#     easyHam = load_data('easyHam_clean.txt')\n",
    "#     hardHam = load_data('hardHam_clean.txt')\n",
    "#     spam = load_data('spam_clean.txt')\n",
    "#     return easyHam,hardHam,spam\n",
    "data= pd.read_csv(\"dataset/spam.csv\",encoding = 'latin-1')\n",
    "data.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)\n",
    "data.rename(columns = {'v1':'class_label','v2':'message'},inplace=True)\n",
    "data.head()\n",
    "data['length'] = data['message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ham  = data[data['class_label'] == \"ham\"].copy()\n",
    "data_spam = data[data['class_label'] == \"spam\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class_label'] = data['class_label'].map( {'spam': 1, 'ham': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=data['message'].tolist()\n",
    "messages_clean=[]\n",
    "for i in messages:\n",
    "    messages_clean.append(preprocessing_data(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n"
     ]
    }
   ],
   "source": [
    "print(len(messages_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data['class_label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easyHam,hardHam,spam=readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # datas=easyHam+hardHam+spam\n",
    "# datas=hardHam+spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # labels = [0] * len(easyHam)+[0]*len(hardHam)+[1]*len(spam)\n",
    "# labels = [0]*len(hardHam)+[1]*len(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hàm trộn theo tỷ lệ cho 1 nhãn, output = train_set,test_set theo nhãn\n",
    "def shuffle_data(mail_by_label,ratio):\n",
    "    tmp=mail_by_label.copy()\n",
    "    shuffle(tmp)\n",
    "    index=int(len(mail_by_label)*ratio)\n",
    "#     print (index,len(tmp))\n",
    "    return tmp[:index],tmp[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm chia tỷ lệ, mails = data, lables là mảng nhãn ratio là tỷ lệ , input (mails,labels, tỷ lệ), output -> train_set,test_set\n",
    "# Hàm defaultdict tạo ra kiểu dữ liệu key - value. Trùng key sẽ nối danh sách liên kết vào value\n",
    "# Hàm zip () :input 2 mảng mails và lables cùng kích thước -> output 1 mảng, mỗi phần tử của mảng gồm 2 phần tử của mail và lable\n",
    "# Hàm set (): VD input [0,0,0,0,0,0,0,1,1,1,1,1,1,2,2,3,3] -> output [0,1,2,3]\n",
    "def split_data(mails,labels,ratio):\n",
    "    split_data_by_labels=defaultdict(list)\n",
    "    for mail,label in zip(mails,labels):\n",
    "        split_data_by_labels[label].append(mail)\n",
    "    train,test=[],[]\n",
    "    for label in set(labels):\n",
    "        data_train,data_test=shuffle_data(split_data_by_labels[label],ratio)\n",
    "        label_train=[label]*len(data_train)\n",
    "        label_test=[label]*len(data_test)\n",
    "        train.extend(zip(label_train,data_train))\n",
    "        test.extend(zip(label_test,data_test))\n",
    "    data_train = [item[1] for item in train]\n",
    "    label_train =[item[0] for item in train]\n",
    "    data_test =[item[1] for item in test]\n",
    "    label_test =[item[0] for item in test]\n",
    "    return data_train,data_test,label_train,label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457\n",
      "1115\n"
     ]
    }
   ],
   "source": [
    "# data_train,data_test,label_train,label_test=split_data(datas,labels,0.4)\n",
    "data_train,data_test,label_train,label_test=split_data(messages_clean,labels,0.8)\n",
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nhóm email theo nhẫn lớp\n",
    "def split_by_label(mails, labels):\n",
    "    split_data_by_labels=defaultdict(list)\n",
    "    for mail,label in zip(mails,labels):\n",
    "        split_data_by_labels[label].append(mail)\n",
    "    return split_data_by_labels\n",
    "# Tính tần suất xuất hiện của từ trong từng nhãn lớp, và số email của từng nhẫn lớp\n",
    "def create_Dir(data_train,label_train):\n",
    "    split_data_by_labels = split_by_label(data_train,label_train)\n",
    "    spam_freq=word_freq(split_data_by_labels[1])\n",
    "    ham_freq=word_freq(split_data_by_labels[0])\n",
    "    total_spam_emails = len(split_data_by_labels[1])\n",
    "    total_ham_emails = len(split_data_by_labels[0])\n",
    "    return spam_freq,ham_freq,total_spam_emails,total_ham_emails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   spam_prob  ham_prob\n",
      "0   0.133947  0.866053\n"
     ]
    }
   ],
   "source": [
    "#tính xác suất\n",
    "def cal_prob(data_train,label_train):\n",
    "    word_in_spam_freq, word_in_ham_freq,total_spam_emails,total_ham_emails = create_Dir(data_train,label_train)\n",
    "    #số lần xuất hiện của các từ trong từng nhãn lớp\n",
    "    total_word_in_spam = sum( word_in_spam_freq.values())\n",
    "    total_word_in_ham = sum( word_in_ham_freq.values())\n",
    "    #giá trị tuyệt đối của T\n",
    "    spam_words = set(word_in_spam_freq.keys())\n",
    "    ham_words = set(word_in_ham_freq.keys())\n",
    "    dictionary = spam_words.union(ham_words)\n",
    "    #xác suất trước của P(ci)\n",
    "    total_emails= total_spam_emails + total_ham_emails\n",
    "    spam_prob = total_spam_emails/total_emails\n",
    "    ham_prob = total_ham_emails/total_emails\n",
    "    #xác suất của từ khóa trong từ điển đối với hai nhãn\n",
    "    word_in_spam_probs, word_in_ham_probs = Counter(),Counter()\n",
    "    for  word in dictionary:\n",
    "        word_in_spam_probs[word] = (word_in_spam_freq[word]+1)/(total_word_in_spam+len(dictionary))\n",
    "        word_in_ham_probs[word] = (word_in_ham_freq[word]+1)/(total_word_in_ham +len(dictionary))\n",
    "    data_ham_probs = pd.DataFrame.from_dict(word_in_ham_probs, orient='index').reset_index()\n",
    "    data_ham_probs = data_ham_probs.rename(columns={'index':'words', 0:'values'})\n",
    "    data_spam_probs= pd.DataFrame.from_dict(word_in_spam_probs, orient='index').reset_index()\n",
    "    data_spam_probs=data_spam_probs.rename(columns={'index':'words', 0:'values'})\n",
    "    data_ham_probs.to_csv('data_ham_probs.csv', index = False, header=True)\n",
    "    data_spam_probs.to_csv('data_spam_probs.csv', index = False, header=True) \n",
    "    df_pre_prob= pd.DataFrame( {'spam_prob':[spam_prob],'ham_prob':[ham_prob]})\n",
    "    df_pre_prob.to_csv('data_pre_probs.csv', index = False, header=True)\n",
    "    print(df_pre_prob)\n",
    "#     return spam_prob, ham_prob, word_in_spam_probs, word_in_ham_probs\n",
    "    \n",
    "cal_prob(data_train,label_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    data_ham_probs= pd.read_csv(\"data_ham_probs.csv\") \n",
    "    data_spam_probs= pd.read_csv(\"data_spam_probs.csv\")\n",
    "    data_pre_prob= pd.read_csv(\"data_pre_probs.csv\")\n",
    "    # lấy xác suất từ files\n",
    "    word_in_ham_probs= data_ham_probs.set_index('words')['values'].to_dict()\n",
    "    word_in_spam_probs= data_spam_probs.set_index('words')['values'].to_dict()\n",
    "    spam_pre_prob = data_pre_prob['spam_prob'][0]\n",
    "    ham_pre_prob = data_pre_prob['ham_prob'][0]\n",
    "    return  word_in_ham_probs, word_in_spam_probs, spam_pre_prob, ham_pre_prob\n",
    "def classify(mail, word_in_ham_probs, word_in_spam_probs, spam_pre_prob, ham_pre_prob):\n",
    "\n",
    "#     # lấy tập từ\n",
    "#     spam_words = set(word_in_spam_probs.keys())\n",
    "#     ham_words = set(word_in_ham_probs.keys())\n",
    "#     dictionary = spam_words.union(ham_words)\n",
    "    #tính xác suất\n",
    "    mail=preprocessing_data(mail)\n",
    "    mail_words = mail.split(' ')\n",
    "    prob_spam = prob_ham = 0.0\n",
    "    for mail_word in set(mail_words):\n",
    "        if(word_in_ham_probs.get(mail_word) is not None):\n",
    "            prob_ham += log(word_in_ham_probs.get(mail_word))\n",
    "        if(word_in_spam_probs.get(mail_word) is not None):\n",
    "            prob_spam += log(word_in_spam_probs.get(mail_word))\n",
    "    prob_ham += log( ham_pre_prob)\n",
    "    prob_spam += log( spam_pre_prob)\n",
    "\n",
    "    if prob_spam > prob_ham:\n",
    "        return 1\n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_mails(data_test):    \n",
    "    result=[]\n",
    "    word_in_ham_probs, word_in_spam_probs, spam_pre_prob, ham_pre_prob = load_model()\n",
    "    for i in range(len(data_test)):\n",
    "        result.append(classify(data_test[i], word_in_ham_probs, word_in_spam_probs, spam_pre_prob, ham_pre_prob))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels=pre_mails(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(label_test,predicted_labels):\n",
    "    correct = sum([1 if label_test == predicted_label else 0\n",
    "                    for label_test, predicted_label in zip(label_test, predicted_labels)])\n",
    "    return correct / len(label_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.39910313901345\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(label_test,predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(label_test,predicted_labels, label):\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    for label_test, predicted_label in zip(label_test, predicted_labels):\n",
    "        if label_test == label and predicted_label == label:\n",
    "            true_positive += 1\n",
    "        elif label_test != label and predicted_label != label:\n",
    "            true_negative += 1\n",
    "        elif label_test == label and predicted_label != label:\n",
    "            false_negative += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "    return (true_positive, false_negative, false_positive, true_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_0,false_negative_0,false_positive_0,true_negative_0=confusion_matrix(label_test,predicted_labels,0)\n",
    "true_positive_1,false_negative_1,false_positive_1,true_negative_1=confusion_matrix(label_test,predicted_labels,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision_0=true_positive_0/(true_positive_0+false_positive_0)\n",
    "Precision_1=true_positive_1/(true_positive_1+false_positive_1)\n",
    "Recall_0=true_positive_0/(true_positive_0+false_negative_0)\n",
    "Recall_1=true_positive_1/(true_positive_1+false_negative_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824742268041237"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9172413793103448"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9875647668393782"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Recall_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8866666666666667"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Recall_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_0=(2*Precision_0*Recall_0)/(Precision_0+Recall_0)\n",
    "F1_1=(2*Precision_1*Recall_1)/(Precision_1+Recall_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9850129198966409\n",
      "0.9016949152542374\n"
     ]
    }
   ],
   "source": [
    "print(F1_0)\n",
    "print(F1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
