{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "# Create 3 files : hardHam_clean, easyHam_clean, spam_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Game\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import os\n",
    "from text_preprocessing import preprocess_text\n",
    "from text_preprocessing import (\n",
    "    to_lower,\n",
    "    remove_email, \n",
    "    remove_url, \n",
    "    remove_punctuation, \n",
    "    lemmatize_word, \n",
    "    remove_number, \n",
    "    stem_word, \n",
    "    remove_stopword,\n",
    "    remove_whitespace\n",
    ")\n",
    "# Thu vien email.parser dung cho tep file co cau truc gioi tin email http\n",
    "from email.parser import Parser\n",
    "\n",
    "### Lay duong dan den tat ca cac file trong folder\n",
    "def read_files_name(folderPath):\n",
    "    result = []\n",
    "    for dirPath, dirs, files in os.walk(folderPath):\n",
    "        for fileName in files:\n",
    "            fName = os.path.join(dirPath, fileName)\n",
    "            result.append(fName)\n",
    "    return result\n",
    "\n",
    "### Tra ve 3 mang chua danh sach ten file trong thu muc\n",
    "def read_forders_Path():\n",
    "    easyHam = read_files_name('dataset\\easy_ham')\n",
    "    hardHam = read_files_name('dataset\\hard_ham')\n",
    "    spam = read_files_name('dataset\\spam')\n",
    "    return easyHam, hardHam, spam\n",
    "\n",
    "\n",
    "def strip_url(mess):\n",
    "    return re.sub(r'http[s]?://\\S+|www\\.\\S+', ' ', mess)\n",
    "\n",
    "\n",
    "def strip_html(mess):\n",
    "    # clean html_tag\n",
    "    clean = re.compile('<.*?>')\n",
    "    mess = re.sub(clean, ' ', mess)\n",
    "    # clean html_space_white\n",
    "    return re.sub('&nbsp;', ' ', mess)\n",
    "\n",
    "\n",
    "\n",
    "def strip_email_header(message):\n",
    "    message = Parser().parsestr(message)\n",
    "    subject = message.get('subject', '')\n",
    "    body = get_email_body(message)\n",
    "\n",
    "    return '\\n'.join((subject, body))\n",
    "\n",
    "def get_email_body(message):\n",
    "    payloads = message.get_payload()\n",
    "    if isinstance(payloads, list):\n",
    "        return '\\n'.join([get_email_body(message) for message in payloads])\n",
    "    elif isinstance(payloads, str):\n",
    "        return payloads\n",
    "\n",
    "\n",
    "def preprocessing_data(mess):\n",
    "    # processing_function_list = [\n",
    "    #     str.lower,\n",
    "    #     strip_url,\n",
    "    #     remove_url,\n",
    "    #     strip_html,\n",
    "    #     remove_email,\n",
    "    #     remove_number,\n",
    "    #     remove_stopword,\n",
    "    #     remove_punctuation,\n",
    "    #     remove_whitespace\n",
    "\n",
    "    # ]\n",
    "\n",
    "    preprocess_functions = [strip_email_header, to_lower, remove_email, remove_url,strip_url,strip_html, \n",
    "                            remove_number, remove_punctuation, remove_stopword, lemmatize_word]\n",
    "    aftermess = preprocess_text(mess, preprocess_functions)\n",
    "\n",
    "    # return ' '.join(aftermess)\n",
    "    return aftermess\n",
    "\n",
    "def handler():\n",
    "    easyHam,hardHam,spam = read_forders_Path()\n",
    "    count=0\n",
    "#     with open(a[0], 'r', encoding='latin') as fi:\n",
    "#         print(preprocessing_data(fi.read()))\n",
    "    f=open('dataset\\easyHam_clean.txt','a+',encoding='utf-8')\n",
    "#     print(len(easyHam))\n",
    "    for i in easyHam:\n",
    "        try:\n",
    "            with open(i, 'r', encoding='latin') as fi:\n",
    "                temp=preprocessing_data(fi.read())\n",
    "            f.write(temp)\n",
    "#                print(preprocessing_data(fi.read())) \n",
    "            f.write('\\n')\n",
    "        except Exception as e:\n",
    "            print(i + \"Exception :\" + str(e))\n",
    "            count+=1\n",
    "            continue\n",
    "    f.close()\n",
    "    ### hardHam\n",
    "    f=open('dataset\\hardHam_clean.txt','a+',encoding='utf-8')\n",
    "    for i in hardHam:\n",
    "        try:\n",
    "            with open(i, 'r', encoding='latin') as fi:\n",
    "                temp=preprocessing_data(fi.read())\n",
    "            f.write(temp)\n",
    "#                print(preprocessing_data(fi.read())) \n",
    "            f.write('\\n')\n",
    "        except Exception as e:\n",
    "            print(i + \"Exception :\" + str(e))\n",
    "            count+=1\n",
    "            continue\n",
    "    f.close()\n",
    "    \n",
    "    ### spam \n",
    "    f=open('dataset\\spam_clean.txt','a+',encoding='utf-8')\n",
    "    for i in spam:\n",
    "        try:\n",
    "            with open(i, 'r', encoding='latin') as fi:\n",
    "                temp=preprocessing_data(fi.read())\n",
    "            f.write(temp)\n",
    "#                print(preprocessing_data(fi.read())) \n",
    "            f.write('\\n')\n",
    "        except Exception as e:\n",
    "            print(i + \"Exception :\" + str(e))\n",
    "            count+=1\n",
    "            continue\n",
    "    f.close()\n",
    "    print(\"End Game\\n\")\n",
    "#     for i in hardHam:\n",
    "#         f=open('dataset\\hardHam_clean.txt','w+')\n",
    "#         with open(i, 'r', encoding='latin') as fi:\n",
    "#             f.write(preprocessing_data(fi.read()))\n",
    "#         f.write(' ')\n",
    "#     f.close()\n",
    "#     for i in spam:\n",
    "#         f=open('dataset\\spam_clean.txt','w+')\n",
    "#         with open(i, 'r', encoding='latin') as fi:\n",
    "#             f.write(preprocessing_data(fi.read()))\n",
    "#         f.write(' ')\n",
    "#     f.close()\n",
    "if __name__ == \"__main__\":\n",
    "    handler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thống kê tần suất xuất hiện và tính xác suất trước"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viết 1 hàm: input 1 file, output 1 mảng kiểu map (từ, fre)\n",
    "from collections import Counter, defaultdict\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fileName):\n",
    "    with open('dataset\\\\'+fileName, 'r', encoding='utf-8') as fi:\n",
    "            temp=fi.readlines()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(data_train):\n",
    "    result=Counter()\n",
    "    for email in data_train:\n",
    "        word=email.split(' ')\n",
    "        result.update(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData():\n",
    "    easyHam = load_data('/dataset/easyHam_clean.txt')\n",
    "    hardHam = load_data('/dataset/hardHam_clean.txt')\n",
    "    spam = load_data('spam_clean.txt')\n",
    "    return easyHam,hardHam,spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "easyHam,hardHam,spam=readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas=easyHam+hardHam+spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0] * len(easyHam)+[0]*len(hardHam)+[1]*len(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hàm trộn theo tỷ lệ cho 1 nhãn, output = train_set,test_set theo nhãn\n",
    "def shuffle_data(mail_by_label,ratio):\n",
    "    tmp=mail_by_label.copy()\n",
    "    shuffle(tmp)\n",
    "    index=int(len(mail_by_label)*ratio)\n",
    "    print (index,len(tmp))\n",
    "    return tmp[:index],tmp[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm chia tỷ lệ, mails = data, lables là mảng nhãn ratio là tỷ lệ , input (mails,labels, tỷ lệ), output -> train_set,test_set\n",
    "# Hàm defaultdict tạo ra kiểu dữ liệu key - value. Trùng key sẽ nối danh sách liên kết vào value\n",
    "# Hàm zip () :input 2 mảng mails và lables cùng kích thước -> output 1 mảng, mỗi phần tử của mảng gồm 2 phần tử của mail và lable\n",
    "# Hàm set (): VD input [0,0,0,0,0,0,0,1,1,1,1,1,1,2,2,3,3] -> output [0,1,2,3]\n",
    "def split_data(mails,labels,ratio):\n",
    "    split_data_by_labels=defaultdict(list)\n",
    "    for mail,label in zip(mails,labels):\n",
    "        split_data_by_labels[label].append(mail)\n",
    "    train,test=[],[]\n",
    "    for label in set(labels):\n",
    "        data_train,data_test=shuffle_data(split_data_by_labels[label],ratio)\n",
    "        label_train=[label]*len(data_train)\n",
    "        label_test=[label]*len(data_test)\n",
    "        train.extend(zip(label_train,data_train))\n",
    "        test.extend(zip(label_test,data_test))\n",
    "    data_train = [item[1] for item in train]\n",
    "    label_train =[item[0] for item in train]\n",
    "    data_test =[item[1] for item in test]\n",
    "    label_test =[item[0] for item in test]\n",
    "    return data_train,data_test,label_train,label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3475 6951\n",
      "1896 3793\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data_train,data_test,label_train,label_test=split_data(datas,labels,0.5)\n",
    "print(label_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-30d775a6b31e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#     return spam_prob, ham_prob, word_in_spam_probs, word_in_ham_probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mcal_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-30d775a6b31e>\u001b[0m in \u001b[0;36mcal_prob\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mdf_ham_probs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_ham_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mdf_spam_probs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_spam_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mdf_ham_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df_ham_probs.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mdf_spam_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df_spam_probs.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m             )\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    354\u001b[0m         )\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mlibwriters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_csv_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mpandas\\_libs\\writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#nhóm email theo nhẫn lớp\n",
    "def split_by_label(mails, labels):\n",
    "    split_data_by_labels=defaultdict(list)\n",
    "    for mail,label in zip(mails,labels):\n",
    "        split_data_by_labels[label].append(mail)\n",
    "    return split_data_by_labels\n",
    "\n",
    "# Tính tần suất xuất hiện của từ trong từng nhãn lớp, và số email của từng nhẫn lớp\n",
    "def create_Dir():\n",
    "    split_data_by_labels = split_by_label(data_train,label_train)\n",
    "    spam_freq=word_freq(split_data_by_labels[1])\n",
    "    ham_freq=word_freq(split_data_by_labels[0])\n",
    "    total_spam_emails = len(split_data_by_labels[1])\n",
    "    total_ham_emails = len(split_data_by_labels[0])\n",
    "    return spam_freq,ham_freq,total_spam_emails,total_ham_emails \n",
    "#tính xác suất\n",
    "def cal_prob():\n",
    "    word_in_spam_freq, word_in_ham_freq,total_spam_emails,total_ham_emails = create_Dir()\n",
    "    #số lần xuất hiện của các từ trong từng nhãn lớp\n",
    "    total_word_in_spam = sum( word_in_spam_freq.values())\n",
    "    total_word_in_ham = sum( word_in_ham_freq.values())\n",
    "    #giá trị tuyệt đối của T\n",
    "    spam_words = set(word_in_spam_freq.keys())\n",
    "    ham_words = set(word_in_ham_freq.keys())\n",
    "    dictionary = spam_words.union(ham_words)\n",
    "    #xác suất trước của P(ci)\n",
    "    total_emails= total_spam_emails + total_ham_emails\n",
    "    spam_prob = total_spam_emails/total_emails\n",
    "    ham_prob = total_ham_emails/total_emails\n",
    "    #xác suất của từ khóa trong từ điển đối với hai nhãn\n",
    "    word_in_spam_probs, word_in_ham_probs = Counter(),Counter()\n",
    "    for  word in dictionary:\n",
    "        word_in_spam_probs[word] = (word_in_spam_freq[word]+1)/(total_word_in_spam+len(dictionary))\n",
    "        word_in_ham_probs[word] = (word_in_ham_freq[word]+1)/(total_word_in_ham +len(dictionary))\n",
    "    \n",
    "    data_ham_probs={'words':word_in_ham_probs.keys(), 'values':word_in_ham_probs.values()}\n",
    "    data_spam_probs={'words':word_in_spam_probs.keys(), 'values':word_in_spam_probs.values()}\n",
    "    df_ham_probs= pd.DataFrame(data_ham_probs)\n",
    "    df_spam_probs= pd.DataFrame(data_spam_probs)\n",
    "    df_ham_probs.to_csv('df_ham_probs.csv', index = False, header=True)\n",
    "    df_spam_probs.to_csv('df_spam_probs.csv', index = False, header=True)\n",
    "    \n",
    "    df_pre_prob= pd.DataFrame( {'spam_prob':[spam_prob],'ham_prob':[ham_prob]})\n",
    "    df_pre_prob.to_csv('df_pre_probs.csv', index = False, header=True)\n",
    "#     return spam_prob, ham_prob, word_in_spam_probs, word_in_ham_probs\n",
    "    \n",
    "cal_prob()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    data_ham_probs= pd.read_csv(\"df_ham_probs.csv\") \n",
    "    data_spam_probs= pd.read_csv(\"df_spam_probs.csv\")\n",
    "    data_pre_prob= pd.read_csv(\"df_pre_probs.csv\")\n",
    "    # lấy xác suất từ files\n",
    "    word_in_ham_probs= data_ham_probs.set_index('words')['values'].to_dict()\n",
    "    word_in_spam_probs= data_spam_probs.set_index('words')['values'].to_dict()\n",
    "    spam_pre_prob = data_pre_prob['spam_prob'][0]\n",
    "    ham_pre_prob = data_pre_prob['ham_prob'][0]\n",
    "    return  word_in_ham_probs, word_in_spam_probs, spam_pre_prob, ham_pre_prob\n",
    "def classify(mail, word_in_ham_probs, word_in_spam_probs, spam_pre_prob, ham_pre_prob):\n",
    "\n",
    "#     # lấy tập từ\n",
    "#     spam_words = set(word_in_spam_probs.keys())\n",
    "#     ham_words = set(word_in_ham_probs.keys())\n",
    "#     dictionary = spam_words.union(ham_words)\n",
    "    #tính xác suất\n",
    "    mail_words = mail.split(' ')\n",
    "    prob_spam = prob_ham = 0.0\n",
    "    for mail_word in mail_words:\n",
    "        if(word_in_ham_probs.get(mail_word) is not None):\n",
    "            prob_ham += log(word_in_ham_probs.get(mail_word))\n",
    "        if(word_in_spam_probs.get(mail_word) is not None):\n",
    "            prob_spam += log(word_in_spam_probs.get(mail_word))\n",
    "    prob_ham += log( ham_pre_prob)\n",
    "    prob_spam += log( spam_pre_prob)\n",
    "\n",
    "    if prob_spam > prob_ham:\n",
    "        return 1\n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "word_in_ham_probs, word_in_spam_probs, spam_pre_prob, ham_pre_prob = load_model()\n",
    "for i in range(len(data_test)):\n",
    "    if (classify(data_test[i], word_in_ham_probs, word_in_spam_probs, spam_pre_prob, ham_pre_prob) == label_test[i]):\n",
    "        correct+=1\n",
    "print( correct)\n",
    "print(len(data_test))\n",
    "print(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
