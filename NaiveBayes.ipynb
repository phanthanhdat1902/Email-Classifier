{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Game\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import os\n",
    "from text_preprocessing import preprocess_text\n",
    "from text_preprocessing import (\n",
    "    to_lower,\n",
    "    remove_email, \n",
    "    remove_url, \n",
    "    remove_punctuation, \n",
    "    lemmatize_word, \n",
    "    remove_number, \n",
    "    stem_word, \n",
    "    remove_stopword,\n",
    "    remove_whitespace\n",
    ")\n",
    "# Thu vien email.parser dung cho tep file co cau truc gioi tin email http\n",
    "from email.parser import Parser\n",
    "\n",
    "# ### Lay duong dan den tat ca cac file trong folder\n",
    "# def read_files_name(folderPath):\n",
    "#     result = []\n",
    "#     for dirPath, dirs, files in os.walk(folderPath):\n",
    "#         for fileName in files:\n",
    "#             fName = os.path.join(dirPath, fileName)\n",
    "#             result.append(fName)\n",
    "#     return result\n",
    "\n",
    "# ### Tra ve 3 mang chua danh sach ten file trong thu muc\n",
    "# def read_forders_Path():\n",
    "#     easyHam = read_files_name('dataset\\easy_ham')\n",
    "#     hardHam = read_files_name('dataset\\hard_ham')\n",
    "#     spam = read_files_name('dataset\\spam')\n",
    "#     return easyHam, hardHam, spam\n",
    "\n",
    "\n",
    "# def strip_url(mess):\n",
    "#     return re.sub(r'http[s]?://\\S+|www\\.\\S+', ' ', mess)\n",
    "\n",
    "\n",
    "# def strip_html(mess):\n",
    "#     # clean html_tag\n",
    "#     clean = re.compile('<.*?>')\n",
    "#     mess = re.sub(clean, ' ', mess)\n",
    "#     # clean html_space_white\n",
    "#     return re.sub('&nbsp;', ' ', mess)\n",
    "\n",
    "\n",
    "\n",
    "# def strip_email_header(message):\n",
    "#     message = Parser().parsestr(message)\n",
    "#     subject = message.get('subject', '')\n",
    "#     body = get_email_body(message)\n",
    "\n",
    "#     return '\\n'.join((subject, body))\n",
    "\n",
    "# def get_email_body(message):\n",
    "#     payloads = message.get_payload()\n",
    "#     if isinstance(payloads, list):\n",
    "#         return '\\n'.join([get_email_body(message) for message in payloads])\n",
    "#     elif isinstance(payloads, str):\n",
    "#         return payloads\n",
    "\n",
    "\n",
    "# def preprocessing_data(mess):\n",
    "#     # processing_function_list = [\n",
    "#     #     str.lower,\n",
    "#     #     strip_url,\n",
    "#     #     remove_url,\n",
    "#     #     strip_html,\n",
    "#     #     remove_email,\n",
    "#     #     remove_number,\n",
    "#     #     remove_stopword,\n",
    "#     #     remove_punctuation,\n",
    "#     #     remove_whitespace\n",
    "\n",
    "#     # ]\n",
    "\n",
    "# #     preprocess_functions = [strip_email_header, to_lower, remove_email, remove_url,strip_url,strip_html, \n",
    "# #                             remove_number, remove_punctuation, remove_stopword, lemmatize_word]\n",
    "#     preprocess_functions = [strip_email_header, to_lower, remove_email, remove_url, \n",
    "#                             remove_number, remove_punctuation, remove_stopword, lemmatize_word]\n",
    "#     aftermess = preprocess_text(mess, preprocess_functions)\n",
    "\n",
    "#     # return ' '.join(aftermess)\n",
    "#     return aftermess\n",
    "\n",
    "# def handler():\n",
    "#     easyHam,hardHam,spam = read_forders_Path()\n",
    "#     count=0\n",
    "# #     with open(a[0], 'r', encoding='latin') as fi:\n",
    "# #         print(preprocessing_data(fi.read()))\n",
    "#     f=open('dataset\\easyHam_clean.txt','a+',encoding='utf-8')\n",
    "# #     print(len(easyHam))\n",
    "#     for i in easyHam:\n",
    "#         try:\n",
    "#             with open(i, 'r', encoding='latin') as fi:\n",
    "#                 temp=preprocessing_data(fi.read())\n",
    "#             f.write(temp)\n",
    "# #                print(preprocessing_data(fi.read())) \n",
    "#             f.write('\\n')\n",
    "#         except Exception as e:\n",
    "#             print(i + \"Exception :\" + str(e))\n",
    "#             count+=1\n",
    "#             continue\n",
    "#     f.close()\n",
    "#     ### hardHam\n",
    "#     f=open('dataset\\hardHam_clean.txt','a+',encoding='utf-8')\n",
    "#     for i in hardHam:\n",
    "#         try:\n",
    "#             with open(i, 'r', encoding='latin') as fi:\n",
    "#                 temp=preprocessing_data(fi.read())\n",
    "#             f.write(temp)\n",
    "# #                print(preprocessing_data(fi.read())) \n",
    "#             f.write('\\n')\n",
    "#         except Exception as e:\n",
    "#             print(i + \"Exception :\" + str(e))\n",
    "#             count+=1\n",
    "#             continue\n",
    "#     f.close()\n",
    "    \n",
    "#     ### spam \n",
    "#     f=open('dataset\\spam_clean.txt','a+',encoding='utf-8')\n",
    "#     for i in spam:\n",
    "#         try:\n",
    "#             with open(i, 'r', encoding='latin') as fi:\n",
    "#                 temp=preprocessing_data(fi.read())\n",
    "#             f.write(temp)\n",
    "# #                print(preprocessing_data(fi.read())) \n",
    "#             f.write('\\n')\n",
    "#         except Exception as e:\n",
    "#             print(i + \"Exception :\" + str(e))\n",
    "#             count+=1\n",
    "#             continue\n",
    "#     f.close()\n",
    "#     print(\"End Game\\n\")\n",
    "# #     for i in hardHam:\n",
    "# #         f=open('dataset\\hardHam_clean.txt','w+')\n",
    "# #         with open(i, 'r', encoding='latin') as fi:\n",
    "# #             f.write(preprocessing_data(fi.read()))\n",
    "# #         f.write(' ')\n",
    "# #     f.close()\n",
    "# #     for i in spam:\n",
    "# #         f=open('dataset\\spam_clean.txt','w+')\n",
    "# #         with open(i, 'r', encoding='latin') as fi:\n",
    "# #             f.write(preprocessing_data(fi.read()))\n",
    "# #         f.write(' ')\n",
    "# #     f.close()\n",
    "# if __name__ == \"__main__\":\n",
    "#     handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viết 1 hàm: input 1 file, output 1 mảng kiểu map (từ, fre)\n",
    "from collections import Counter, defaultdict\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fileName):\n",
    "    with open('dataset\\\\'+fileName, 'r', encoding='utf-8') as fi:\n",
    "            temp=fi.readlines()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(data_train):\n",
    "    result=Counter()\n",
    "    for email in data_train:\n",
    "        word=email.split(' ')\n",
    "        result.update(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData():\n",
    "    easyHam = load_data('easyHam_clean.txt')\n",
    "    hardHam = load_data('hardHam_clean.txt')\n",
    "    spam = load_data('spam_clean.txt')\n",
    "    return easyHam,hardHam,spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "easyHam,hardHam,spam=readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas=hardHam+spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0]*len(hardHam)+[1]*len(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8586x85425 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1661830 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test=  train_test_split(datas,labels, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    input='content',     # input is actual text\n",
    "    lowercase=True,      # convert to lower case before tokenizing\n",
    "    stop_words='english' # remove stop words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_transformed = vectorizer.fit_transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_transformed  = vectorizer.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(features_train_transformed, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier accuracy 89.87%\n"
     ]
    }
   ],
   "source": [
    "print(\"classifier accuracy {:.2f}%\".format(classifier.score(features_test_transformed, labels_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(vectorizer.transform(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.87194412107101 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", 100 * sum(predictions == labels_test) / len(predictions), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
